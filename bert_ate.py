# -*- coding: utf-8 -*-
"""BERT ATE

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/130XpXhyK1kxsfpK4zTAGYr_9pXQ7_CjC

#Importy
"""

!pip install pytorch_lightning
!pip install torchmetrics
!pip install transformers

import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import torch
import torchmetrics
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, TensorDataset
from transformers import BertTokenizerFast as BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup, AutoTokenizer, DataCollatorForTokenClassification, AutoModelForTokenClassification
import pytorch_lightning as pl
import nltk


from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
from pytorch_lightning.loggers import TensorBoardLogger
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, multilabel_confusion_matrix

RANDOM_SEED = 42

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

"""#Dataset"""

from google.colab import drive
drive.mount("/content/gdrive")

SEMEVAL_TRAIN = '/content/gdrive/My Drive/SemEval2014/Restaurants_Train.xml'
SEMEVAL_TEST = '/content/gdrive/My Drive/SemEval2014/Restaurants_Test_Data_phaseB.xml'

"""#Labels"""

from nltk.tokenize import word_tokenize, wordpunct_tokenize
import xml.etree.cElementTree as et

nltk.download('stopwords')
nltk.download('punkt')

#TODO Obsługa literówek
#DONE Obsługa termów, które są rozmieszczone na kilka tokenów np. Shabu-Shabu -> 'Shabu', '-', 'Shabu'
#TODO Obsługa słów, które występują w więcej niż jednym aspectTerm np ['pizza', 'thin pizza crust'] jeden token 'pizza'

def parse_aspect_terms(data_path):
  tree=et.parse(data_path)
  root=tree.getroot()

  aspectTerms = []
  tokens = []
  labels = []
  for sentence in root.iter('sentence'):
    aspectTermSent = []
    root1 = et.Element('root')
    root1 = sentence

    text = sentence.find('text').text
    tokensSent = word_tokenize(text)
    labelsSent = np.zeros(len(tokensSent), dtype=np.int64)

    for aspectTerm in root1.iter('aspectTerm'):
      root2 = et.Element('root')
      root2 = (aspectTerm)

      term = root2.get('term')
      termWords = word_tokenize(term)
      count = 0
      for t in termWords:
        if t in tokensSent:
          index = tokensSent.index(t)
          if labelsSent[index] != 0:
            index = tokensSent.index(t, index+1)
          if (count == 0):
            labelsSent[index] = 1
          else:
            labelsSent[index] = 2
        else:
          print(t +' not in '+';'.join(tokensSent)+'\n')
        count += 1

      aspectTermSent.append(root2.get('term'))

    labels.append(labelsSent)
    tokens.append(tokensSent)
    aspectTerms.append(aspectTermSent)

  return aspectTerms, tokens, labels

def read_dataset(data_path, one_hot = True):
  aspect_terms, tokens, labels = parse_aspect_terms(data_path)
  raw_data = pd.read_xml(data_path)
  # print(raw_data.shape)
  data = raw_data.drop(['aspectTerms', 'aspectCategories'], axis=1)
  data['aspectTerms'] = aspect_terms
  data['tokens'] = tokens
  data['labels'] = labels

  return data

dataset_labeled = read_dataset(SEMEVAL_TRAIN)
test_data = read_dataset(SEMEVAL_TEST)
train_data, val_data = train_test_split(dataset_labeled, test_size=0.1, random_state= RANDOM_SEED, shuffle=True)
train_data.head()

val_data.head()

test_data.head()

"""
## Tokenization"""

#huggingface

def one_hot_label(label):
  if label == 0:
    return [1,0,0]

  if label == 1:
    return [0,1,0]

  if label == 2:
    return [0,0,1]

def align_labels_with_tokens(labels, word_ids, padding_label=0):
    new_labels = []
    current_word = None
    for word_id in word_ids:
        if word_id != current_word:
            # Start of a new word!
            current_word = word_id
            label = padding_label if word_id is None else labels[word_id]
            new_labels.append(one_hot_label(label))
        elif word_id is None:
            # Special token
            new_labels.append(one_hot_label(padding_label))
        else:
            # Same word as previous token
            label = labels[word_id]
            new_labels.append(one_hot_label(label))

    return new_labels

class TokenizedDataset(Dataset):
  def __init__(
      self,
      dataset,
      tokenizer
  ):
    self.tokenizer = tokenizer
    self.dataset = dataset


  def __len__(self):
    return len(self.dataset)

  def __getitem__(self, index: int):
    row = self.dataset.iloc[index]

    tokenized_inputs = self.tokenizer.encode_plus(
        row["tokens"], max_length=128, padding="max_length",
        return_token_type_ids=False,
        is_split_into_words=True, truncation=True,
        return_tensors='pt')
    new_labels = []

    word_ids = tokenized_inputs.word_ids()
    new_labels = align_labels_with_tokens(row["labels"], word_ids)
    tokenized_inputs["labels"] = torch.FloatTensor(new_labels)
    return dict(
        input_ids = tokenized_inputs["input_ids"].flatten(),
        attention_mask = tokenized_inputs["attention_mask"].flatten(),
        labels = tokenized_inputs["labels"]
    )

BERT_MODEL_NAME = 'bert-base-uncased' # case insensitive
tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)

class DataModule(pl.LightningDataModule):
    def __init__(self, train_df, test_df, val_df, tokenizer, batch_size=8, max_token_len=128):
      super().__init__()
      self.batch_size = batch_size
      self.train_df = train_df
      self.test_df = test_df
      self.val_df = val_df
      self.tokenizer = tokenizer
      self.max_token_len = max_token_len


    def setup(self, stage=None):
      self.train_dataset = TokenizedDataset(
      self.train_df,
      self.tokenizer,
      )

      self.test_dataset = TokenizedDataset(
      self.test_df,
      self.tokenizer,
      )

      self.val_dataset = TokenizedDataset(
      self.val_df,
      self.tokenizer,
      )

    def train_dataloader(self):
      return DataLoader(
        self.train_dataset,
        batch_size=self.batch_size, # how many samples per batch to load (default: 1)
        shuffle=True, # set to True to have the data reshuffled at every epoch (default: False).
        num_workers=2 # how many subprocesses to use for data loading. 0 means that the data will be loaded in the main process. (default: 0)
      )
    def val_dataloader(self):
      return DataLoader(
        self.val_dataset,
        batch_size=self.batch_size,
        num_workers=2
      )
    def test_dataloader(self):
      return DataLoader(
        self.test_dataset,
        batch_size=self.batch_size,
        num_workers=2
      )

"""#Model"""

class BertATE(pl.LightningModule):
  def __init__(self,
               bert,
               num_labels = 3,
               criterion= nn.BCELoss(),
               activation = nn.Sigmoid()):
        super().__init__()
        self.bert = bert
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)
        self.criterion = criterion
        self.activation = activation


  def forward(self, input_ids, attention_mask, labels=None):
      output = self.bert(input_ids = input_ids, attention_mask=attention_mask)
      output = self.classifier(output[0])
      output = self.activation(output) # BERT -> Linear -> Sigmoid
      loss = 0
      if labels is not None:
        loss = self.criterion(output, labels)
      return loss, output

  def training_step(self, batch, batch_idx):
    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    labels = batch["labels"]
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log("train_loss", loss, prog_bar=True, logger=True)
    return {"loss": loss, "predictions": outputs, "labels": labels}

  def validation_step(self, batch, batch_idx):
    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    labels = batch["labels"]
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log("val_loss", loss, prog_bar=True, logger=True)
    return loss

  def test_step(self, batch, batch_idx):
    input_ids = batch["input_ids"]
    attention_mask = batch["attention_mask"]
    labels = batch["labels"]
    loss, outputs = self(input_ids, attention_mask, labels)
    self.log("test_loss", loss, prog_bar=True, logger=True)
    return loss

  def training_epoch_end(self, outputs):
    labels = []
    predictions = []
    for output in outputs:
      for out_labels in output["labels"].detach().cpu():
        labels.append(out_labels)
      for out_predictions in output["predictions"].detach().cpu():
        predictions.append(out_predictions)
    labels = torch.stack(labels).int()
    predictions = torch.stack(predictions)

  def configure_optimizers(self):
    optimizer = AdamW(self.parameters(), lr=2e-5)
    return optimizer

early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)
MODEL_NAME = "bert-base-uncased"

checkpoint_callback = ModelCheckpoint(
    dirpath="checkpoints",
    filename="best-checkpoint",
    save_top_k=1,
    verbose=True, # ?
    monitor="val_loss", # loss function on validation set
    mode="min"
  )
 # https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.TensorBoardLogger.html
logger = TensorBoardLogger("lightning_logs", name="toxic-comments")

modelATE = BertATE(
      BertModel.from_pretrained(MODEL_NAME, return_dict=True))

data_module = DataModule(
  train_data,
  test_data,
  val_data,
  tokenizer,
  batch_size=12
)

trainer = pl.Trainer(
  logger = logger,
  callbacks = [checkpoint_callback, early_stopping_callback],
  max_epochs=10,
  gpus=1
)

trainer.fit(modelATE, data_module)

"""#Eval"""

filename = "checkpoints/best-checkpoint.ckpt"
trained_model = BertATE.load_from_checkpoint(filename,
                                             bert = BertModel.from_pretrained(MODEL_NAME, return_dict=True))

trained_model.to(device)
trained_model.eval()
trained_model.freeze()

test_dataset = TokenizedDataset(
      test_data,
      tokenizer
    )
predictions = []
labels = []
attention_mask = []
for item in tqdm(test_dataset):
      _, prediction = trained_model(
        input_ids = item["input_ids"].unsqueeze(dim=0).to(device),
        attention_mask = item["attention_mask"].unsqueeze(dim=0).to(device)
      )
      predictions.append(prediction.squeeze().tolist())
      labels.append(item["labels"].int().tolist())
      attention_mask.append(item["attention_mask"].tolist())


# predictions = torch.stack(predictions).detach().cpu()
# labels = torch.stack(labels).detach().cpu()
# attention_mask = torch.stack(attention_mask).detach().cpu()

# sample = test_dataset[0]
# _, prediction = trained_model(sample["input_ids"].unsqueeze(dim=0).to(device), sample["attention_mask"].unsqueeze(dim=0).to(device))
# print(prediction)
# print(test_dataset[0]["labels"])

# sample_attention = sample["attention_mask"]
# sample_label = sample["labels"]
# sample_prediction = prediction.detach().cpu()
# true_labels = []
# true_predictions = []

def get_rid_of_mask(output, labels, mask):
  true_predictions = []
  true_labels = []

  for seq_pred, seq_lab, seq_m in zip(output, labels, mask):
    w_pred = []
    w_lab = []

    for tok_pred, tok_lab, m in zip(seq_pred, seq_lab, seq_m):
      if m == 0:
        break
      else:
        w_pred.append(np.argmax(tok_pred))
        w_lab.append(np.argmax(tok_lab))

    true_predictions.append(w_pred)
    true_labels.append(w_lab)

  return true_predictions, true_labels

print(get_rid_of_mask(predictions, labels, attention_mask))

"""https://github.com/NLPWM-WHU/RACL/blame/08bbda1c1798e948a6a66fed069bc2f58bdf01d2/evaluation.py#L5"""

from sklearn.metrics import f1_score

def custom_f1(predictions, labels):

  correct, aTerms, predicted = 0, 0, 0

  for seq in range(len(predictions)):

    seq_lab = labels[seq]
    seq_pred = predictions[seq]

    for term in range(len(seq_lab)):

      if seq_lab[term] == 1:
        aTerms += 1

        if seq_pred[term] == 1:
          matching = True

          for i in range(term+1, len(seq_lab)):
            if seq_lab[i] == 2 and seq_pred[i] == 2:
              continue
            elif seq_lab[i] != 2 and seq_pred[i] != 2:
              break
            else:
              matching = False
              break

          if matching == True:
            correct += 1

    for i in seq_pred:
      if i == 1:
        predicted += 1

  precis = correct/(predicted + 1e-6)
  reca = correct/ (aTerms + 1e-6)
  F_1score = (2 * precis * reca)/(precis + reca + 1e-6)

  return F_1score

pred, lab = get_rid_of_mask(predictions, labels, attention_mask)
print(custom_f1(pred, lab))

"""https://nlpforhackers.io/lstm-pos-tagger-keras/"""

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)
pd.set_option('display.max_colwidth', -1)
# pd.reset_option('all')

display(test_data)